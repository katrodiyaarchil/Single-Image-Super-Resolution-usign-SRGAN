{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN-SGP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ay1deloi-Wg"
      },
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import os\n",
        "from keras import Input\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.applications import VGG19\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense, PReLU, Flatten\n",
        "from keras.layers.convolutional import Conv2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras_preprocessing.image import img_to_array, load_img\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzGu45-tCuRA"
      },
      "source": [
        "residual_blocks = 16\n",
        "momentum =0.8 \n",
        "input_shape = (64,64,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u_Zgra_Djzj"
      },
      "source": [
        "def residual_block(x):\n",
        "  '''\n",
        "  Defining residual block\n",
        "  '''\n",
        "  filters = [64, 64]\n",
        "  kernel_size = 3\n",
        "  strides = 1\n",
        "  padding = \"same\"\n",
        "  momentum = 0.8\n",
        "  activation = \"relu\"\n",
        "  res = Conv2D(filters=filters[0], kernel_size=kernel_size,\n",
        "  strides=strides, padding=padding)(x)\n",
        "  res = Activation(activation=activation)(res)\n",
        "  res = BatchNormalization(momentum=momentum)(res)\n",
        "  res = Conv2D(filters=filters[1], kernel_size=kernel_size,\n",
        "  strides=strides, padding=padding)(res)\n",
        "  res = BatchNormalization(momentum=momentum)(res)\n",
        "  # Add res and x\n",
        "  res = Add()([res, x])\n",
        "  return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuN34J1mHMdF"
      },
      "source": [
        "def build_generator():\n",
        "\n",
        " \"\"\"\n",
        " Create a generator network using the hyperparameter values defined below\n",
        " :return:\n",
        " \"\"\"\n",
        " residual_blocks = 16\n",
        " momentum = 0.8\n",
        " input_shape = (64, 64, 3)\n",
        "# Input Layer of the generator network\n",
        " input_layer = Input(shape=input_shape)\n",
        "# Add the pre-residual block\n",
        " gen1 = Conv2D(filters=64, kernel_size=9, strides=1, padding='same',\n",
        " activation='relu')(input_layer)\n",
        "  # Add 16 residual blocks\n",
        " res = residual_block(gen1)\n",
        " for i in range(residual_blocks - 1):\n",
        "   res = residual_block(res)\n",
        "  # Add the post-residual block\n",
        " gen2 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(res)\n",
        " gen2 = BatchNormalization(momentum=momentum)(gen2)\n",
        "  # Take the sum of the output from the pre-residual block(gen1) and the post-residual block(gen2)\n",
        " gen3 = Add()([gen2, gen1])\n",
        "  # Add an upsampling block\n",
        " gen4 = UpSampling2D(size=2)(gen3)\n",
        " gen4 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen4)\n",
        " gen4 = Activation('relu')(gen4)\n",
        "  # Add another upsampling block\n",
        " gen5 = UpSampling2D(size=2)(gen4)\n",
        " gen5 = Conv2D(filters=256, kernel_size=3, strides=1,\n",
        " padding='same')(gen5)\n",
        " gen5 = Activation('relu')(gen5)\n",
        "# Output convolution layer\n",
        " gen6 = Conv2D(filters=3, kernel_size=9, strides=1, padding='same')(gen5)\n",
        " output = Activation('tanh')(gen6)\n",
        "# Keras model\n",
        " model = Model(inputs=[input_layer], outputs=[output],\n",
        " name='generator')\n",
        " return model\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5lh1CBOJpMx"
      },
      "source": [
        "Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ57vQxBJuqB"
      },
      "source": [
        "def build_discriminator():\n",
        " \"\"\"\n",
        " Create a discriminator network using the hyperparameter values defined below\n",
        " :return:\n",
        " \"\"\"\n",
        "\n",
        " leakyrelu_alpha = 0.2\n",
        " input_shape = (256,256,3)\n",
        "\n",
        "\n",
        "# Firrst input llayer for Disc\n",
        "\n",
        " input_layer = Input(shape=input_shape)\n",
        "\n",
        "# Add convolution blocks\n",
        " dis1 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(input_layer)\n",
        " dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n",
        "\n",
        "\n",
        "# Seven inner blocks for DIsc\n",
        "\n",
        " dis2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(dis1)\n",
        " dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n",
        " dis2 = BatchNormalization(momentum=momentum)(dis2)\n",
        "# Add the third convolution block\n",
        " dis3 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same')(dis2)\n",
        " dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n",
        " dis3 = BatchNormalization(momentum=momentum)(dis3)\n",
        "# Add the fourth convolution block\n",
        " dis4 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis3)\n",
        " dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n",
        " dis4 = BatchNormalization(momentum=0.8)(dis4)\n",
        "# Add the fifth convolution block\n",
        " dis5 = Conv2D(256, kernel_size=3, strides=1, padding='same')(dis4)\n",
        " dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n",
        " dis5 = BatchNormalization(momentum=momentum)(dis5)\n",
        "# Add the sixth convolution block\n",
        " dis6 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(dis5)\n",
        " dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n",
        " dis6 = BatchNormalization(momentum=momentum)(dis6)\n",
        "# Add the seventh convolution block\n",
        " dis7 = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(dis6)\n",
        " dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n",
        " dis7 = BatchNormalization(momentum=momentum)(dis7)\n",
        "# Add the eight convolution block\n",
        " dis8 = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(dis7)\n",
        " dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n",
        " dis8 = BatchNormalization(momentum=momentum)(dis8)\n",
        "\n",
        "\n",
        "# Dense layer with 1024 neurons \n",
        "\n",
        " dis9 = Dense(units=1024)(dis8)\n",
        " dis9 = LeakyReLU(alpha=0.2)(dis9)\n",
        "\n",
        "# Last dense layer with sigmoid activation for prediction\n",
        " output = Dense(units=1, activation='sigmoid')(dis9)\n",
        " # call model\n",
        " model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n",
        " return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s89l6QhsMEan"
      },
      "source": [
        "# Load and extract feature using VGG19 Network\n",
        "\n",
        "def build_vgg():\n",
        " \"\"\"\n",
        " Build the VGG network to extract image features\n",
        " \"\"\"\n",
        " input_shape = (256, 256, 3)\n",
        " # Load a pre-trained VGG19 model trained on 'Imagenet' dataset\n",
        " vgg = VGG19(weights=\"imagenet\", include_top=False)\n",
        " input_layer = Input(shape=input_shape)\n",
        " vgg.outputs = [vgg.layers[9].output]\n",
        " # Extract features\n",
        " features = vgg(input_layer)\n",
        " # Create a Keras model\n",
        " model = Model(inputs=[input_layer], outputs=[features])\n",
        " return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMul68OcMXaB"
      },
      "source": [
        "# Adversarial network for loss\n",
        "\n",
        "def build_adversarial_model(generator, discriminator, vgg):\n",
        " input_low_resolution = Input(shape=(64, 64, 3))\n",
        " fake_hr_images = generator(input_low_resolution)\n",
        " fake_features = vgg(fake_hr_images)\n",
        " discriminator.trainable = False\n",
        " output = discriminator(fake_hr_images)\n",
        " model = Model(inputs=[input_low_resolution],\n",
        " outputs=[output, fake_features])\n",
        " for layer in model.layers:\n",
        "  print(layer.name, layer.trainable)\n",
        "  print(model.summary())\n",
        " return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNhpuju0NrCI"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaKd6gjINvf8"
      },
      "source": [
        "data_dir = \"/content/img_align_celeba/*.*\"\n",
        "epochs = 20000\n",
        "batch_size = 1\n",
        "# Shape of low-resolution and high-resolution images\n",
        "low_resolution_shape = (64, 64, 3)\n",
        "high_resolution_shape = (256, 256, 3)\n",
        "common_optimizer = Adam(0.0002, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuKMpFrgOlMt"
      },
      "source": [
        "# Build and compile VGG19 network\n",
        "vgg = VGG19(weights=\"imagenet\",include_top=False, input_shape=(256, 256, 3))\n",
        "model = Model(inputs=vgg.inputs, outputs=vgg.layers[9].output)\n",
        "vgg = model\n",
        "vgg.trainable = False\n",
        "vgg.compile(loss='mse', optimizer=common_optimizer, metrics=\n",
        " ['accuracy'])\n",
        "\n",
        "\n",
        "# bUild and compile discriminator network\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='mse', optimizer=common_optimizer,\n",
        " metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# build generator\n",
        "\n",
        "generator = build_generator()\n",
        "\n",
        "# create adversarial model by creating two layers\n",
        "\n",
        "input_high_resolution = Input(shape=high_resolution_shape)\n",
        "input_low_resolution = Input(shape=low_resolution_shape)\n",
        "\n",
        "\n",
        "# Generate high resolution using Generator\n",
        "generated_high_resolution_images = generator(input_low_resolution)\n",
        "\n",
        "# Use VGG19 to extaract the features\n",
        "features = vgg(generated_high_resolution_images)\n",
        "\n",
        "# make discriminator non  trainable\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Generator prediction on the fake images\n",
        "probs = discriminator(generated_high_resolution_images)\n",
        "\n",
        "# Compile adversarial network\n",
        "\n",
        "adversarial_model = Model([input_low_resolution, input_high_resolution], [probs, features])\n",
        "adversarial_model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1e-3, 1], optimizer=common_optimizer)\n",
        "\n",
        "# Visualize using tensorboard\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n",
        "tensorboard.set_model(generator)\n",
        "tensorboard.set_model(discriminator)\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL4aRPWYYIse"
      },
      "source": [
        "# Code to generate low and high resolution images\n",
        "\n",
        "def sample_images(data_dir, batch_size, high_resolution_shape, low_resolution_shape):\n",
        "\n",
        "\n",
        "# Make a list of all images inside the data directory\n",
        "  all_images = glob.glob(data_dir)\n",
        "  # Choose a random batch of images\n",
        "  images_batch = np.random.choice(all_images, size=batch_size)\n",
        "  low_resolution_images = []\n",
        "  high_resolution_images = []\n",
        "  for img in images_batch:\n",
        "  # Get an ndarray of the current image\n",
        "    img1 = cv2.imread(img)\n",
        "    img1 = np.asarray(img1)\n",
        "    img1 = img1.astype(np.float32)\n",
        "  # Resize the image\n",
        "  from skimage.transform import resize                    \n",
        "  \n",
        "  img1_high_resolution = resize(img1, high_resolution_shape)\n",
        "  img1_low_resolution = resize(img1, low_resolution_shape)\n",
        "  # Do a random flip\n",
        "  if np.random.random() < 0.5:\n",
        "    img1_high_resolution = np.fliplr(img1_high_resolution)\n",
        "    img1_low_resolution = np.fliplr(img1_low_resolution)\n",
        "  high_resolution_images.append(img1_high_resolution)\n",
        "  low_resolution_images.append(img1_low_resolution)\n",
        "  return np.array(high_resolution_images),np.array(low_resolution_images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeNvT4F4Rmex"
      },
      "source": [
        "Starts Epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fbdsQrbRCZW"
      },
      "source": [
        "for epoch in range(epochs):\n",
        " print(\"Epoch:{}\".format(epoch))\n",
        " high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir,\n",
        " batch_size=batch_size,low_resolution_shape=low_resolution_shape,\n",
        " high_resolution_shape=high_resolution_shape)\n",
        "\n",
        "# Code to generate low and high resolution images\n",
        "\n",
        " \n",
        " #print(high_resolution_images.shape, low_resolution_images.shape)\n",
        " # normalization\n",
        "\n",
        " high_resolution_images = high_resolution_images / 127.5 - 1.\n",
        " low_resolution_images = low_resolution_images / 127.5 - 1.\n",
        "\n",
        " generated_high_resolution_images = generator.predict(low_resolution_images)\n",
        "\n",
        " real_labels = np.ones((batch_size, 16, 16, 1))\n",
        " fake_labels = np.zeros((batch_size, 16, 16, 1))\n",
        "\n",
        " d_loss_real = discriminator.train_on_batch(high_resolution_images, real_labels)\n",
        " d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake_labels)\n",
        "\n",
        " d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        " high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size=batch_size,high_resolution_shape=high_resolution_shape,low_resolution_shape=low_resolution_shape)\n",
        "\n",
        " # Normalize images\n",
        " high_resolution_images = high_resolution_images / 127.5 - 1.\n",
        " low_resolution_images = low_resolution_images / 127.5 - 1.\n",
        " image_features = vgg.predict(high_resolution_images)\n",
        "\n",
        " g_loss = adversarial_model.train_on_batch([low_resolution_images, high_resolution_images], [real_labels, image_features])\n",
        " #write_log(tensorboard, 'g_loss', g_loss[0], epoch)\n",
        " #write_log(tensorboard, 'd_loss', d_loss[0], epoch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " if epoch % 100 == 0:\n",
        "  high_resolution_images, low_resolution_images = sample_images(data_dir=data_dir, batch_size = batch_size,\n",
        "  high_resolution_shape=high_resolution_shape,low_resolution_shape=low_resolution_shape)\n",
        "  # Normalize images\n",
        "  high_resolution_images = high_resolution_images / 127.5 - 1.\n",
        "  low_resolution_images = low_resolution_images / 127.5 - 1.\n",
        "\n",
        "  # Generate fake high-resolution images\n",
        "  generated_images = generator.predict_on_batch(low_resolution_images)\n",
        "\n",
        " # Save\n",
        "  for index, img in enumerate(generated_images):\n",
        "    plt.imshow(high_resolution_images[index])\n",
        "    plt.show()\n",
        "    plt.imshow(low_resolution_images[index])\n",
        "    plt.show()\n",
        "    plt.imshow(generated_images[index])\n",
        "    plt.show()\n",
        "    #torchvision.utils.save_image(high_resolution_images[index], \"/content/images_saved/\" + str(index))\n",
        "  #  save_images(low_resolution_images[index], high_resolution_images[index], img, path=\"results/img_{}_{}\".format(epoch, index))\n",
        "if cur_step % display_step == 0 and cur_step > 0:\n",
        "  print('Step {}: SRResNet loss: {:.5f}'.format(cur_step, mean_loss))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}